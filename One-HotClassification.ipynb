{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "escsGkWFtzVj"
      },
      "source": [
        "# **One-Hot Encoding Classifiers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "X_test = []\n",
        "with (open(\"X_test\", \"rb\")) as openfile:\n",
        "    while True:\n",
        "        try:\n",
        "            X_test = pickle.load(openfile)\n",
        "        except EOFError:\n",
        "            break\n",
        "y_test = []\n",
        "with (open(\"y_test\", \"rb\")) as openfile:\n",
        "    while True:\n",
        "        try:\n",
        "            y_test = pickle.load(openfile)\n",
        "        except EOFError:\n",
        "            break\n",
        "X_train = []\n",
        "with (open(\"X_train\", \"rb\")) as openfile:\n",
        "    while True:\n",
        "        try:\n",
        "            X_train = pickle.load(openfile)\n",
        "        except EOFError:\n",
        "            break\n",
        "y_train = []\n",
        "with (open(\"y_train\", \"rb\")) as openfile:\n",
        "    while True:\n",
        "        try:\n",
        "            y_train = pickle.load(openfile)\n",
        "        except EOFError:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#This is done for to one-hot incode the guide sequences\n",
        "for guide in X_train:\n",
        "    for i in range(len(guide)):\n",
        "        if guide[i] == 0:\n",
        "            guide[i] = [1,0,0,0,0]\n",
        "        elif guide[i] == 1:\n",
        "            guide[i] = [0,0,0,0,1]\n",
        "        elif guide[i] == 2:\n",
        "            guide[i] = [0,0,0,1,0]\n",
        "        elif guide[i] == 3:\n",
        "            guide[i] = [0,0,1,0,0]\n",
        "        elif guide[i] == 4:\n",
        "            guide[i] = [0,1,0,0,0]\n",
        "for guide in X_test:\n",
        "    for i in range(len(guide)):\n",
        "        if guide[i] == 0:\n",
        "            guide[i] = [1,0,0,0,0]\n",
        "        elif guide[i] == 1:\n",
        "            guide[i] = [0,0,0,0,1]\n",
        "        elif guide[i] == 2:\n",
        "            guide[i] = [0,0,0,1,0]\n",
        "        elif guide[i] == 3:\n",
        "            guide[i] = [0,0,1,0,0]\n",
        "        elif guide[i] == 4:\n",
        "            guide[i] = [0,1,0,0,0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "X_train = np.array(X_train)\n",
        "nsamples, nx, ny = X_train.shape\n",
        "X_train = X_train.reshape((nsamples,nx*ny))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "X_test = np.array(X_test)\n",
        "nsamples, nx, ny = X_test.shape\n",
        "X_test = X_test.reshape((nsamples,nx*ny))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UpZfDRyvb5t"
      },
      "source": [
        "# **Build Classification models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Q8yCRtbQu5-F"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LswMOe9Y26Nm"
      },
      "source": [
        "**K nearest neighbors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fI6Ni6i3EAy",
        "outputId": "fcd88810-42d7-41d6-d4c4-90d808704f14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model performance for Training set\n",
            "- Accuracy: 0.8237410071942446\n",
            "- MCC: 0.6483637045852658\n",
            "- F1 score: 0.8236451745811304\n",
            "----------------------------------\n",
            "Model performance for Test set\n",
            "- Accuracy: 0.7096774193548387\n",
            "- MCC: 0.41875\n",
            "- F1 score: 0.7096774193548387\n",
            "- ROC AUC: 0.709375\n",
            "- Recall: 0.7096774193548387\n",
            "- Precision: 0.7096774193548387\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=3) # Define classifier\n",
        "knn.fit(X_train, y_train) # Train model\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = knn.predict(X_train)\n",
        "y_test_pred = knn.predict(X_test)\n",
        "\n",
        "# Training set performance\n",
        "knn_train_accuracy = accuracy_score(y_train, y_train_pred) # Calculate Accuracy\n",
        "knn_train_mcc = matthews_corrcoef(y_train, y_train_pred) # Calculate MCC\n",
        "knn_train_f1 = f1_score(y_train, y_train_pred, average='weighted') # Calculate F1-score\n",
        "\n",
        "# Test set performance\n",
        "knn_test_accuracy = accuracy_score(y_test, y_test_pred) # Calculate Accuracy\n",
        "knn_test_mcc = matthews_corrcoef(y_test, y_test_pred) # Calculate MCC\n",
        "knn_test_f1 = f1_score(y_test, y_test_pred, average='weighted') # Calculate F1-score\n",
        "knn_test_ROC = roc_auc_score(y_test, y_test_pred) # Calculate ROC AUC\n",
        "knn_test_recall = recall_score(y_test, y_test_pred, average='weighted') # Calculate Recall\n",
        "knn_test_precision = precision_score(y_test, y_test_pred, average='weighted') # Calculate Precision\n",
        "\n",
        "print('Model performance for Training set')\n",
        "print('- Accuracy: %s' % knn_train_accuracy)\n",
        "print('- MCC: %s' % knn_train_mcc)\n",
        "print('- F1 score: %s' % knn_train_f1)\n",
        "print('----------------------------------')\n",
        "print('Model performance for Test set')\n",
        "print('- Accuracy: %s' % knn_test_accuracy)\n",
        "print('- MCC: %s' % knn_test_mcc)\n",
        "print('- F1 score: %s' % knn_test_f1)\n",
        "print('- ROC AUC: %s' % knn_test_ROC)\n",
        "print('- Recall: %s' % knn_test_recall)\n",
        "print('- Precision: %s' % knn_test_precision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "#save model as pickle file\n",
        "filename = 'knn_model1hot.sav'\n",
        "pickle.dump(knn, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojasbTOn4-x-"
      },
      "source": [
        "**Support vector machine (Radial basis function kernel)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ot6hHeU04-2j",
        "outputId": "b95d2e3a-5cd2-47c7-cdc7-87bbd8b9da52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model performance for Training set\n",
            "- Accuracy: 1.0\n",
            "- MCC: 1.0\n",
            "- F1 score: 1.0\n",
            "----------------------------------\n",
            "Model performance for Test set\n",
            "- Accuracy: 0.5806451612903226\n",
            "- MCC: 0.2712254014483242\n",
            "- F1 score: 0.480877082015602\n",
            "- ROC AUC: 0.5666666666666667\n",
            "- Recall: 0.5806451612903226\n",
            "- Precision: 0.7686318131256952\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm_rbf = SVC(gamma=2, C=1)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = svm_rbf.predict(X_train)\n",
        "y_test_pred = svm_rbf.predict(X_test)\n",
        "\n",
        "# Training set performance\n",
        "svm_rbf_train_accuracy = accuracy_score(y_train, y_train_pred) # Calculate Accuracy\n",
        "svm_rbf_train_mcc = matthews_corrcoef(y_train, y_train_pred) # Calculate MCC\n",
        "svm_rbf_train_f1 = f1_score(y_train, y_train_pred, average='weighted') # Calculate F1-score\n",
        "\n",
        "# Test set performance\n",
        "svm_rbf_test_accuracy = accuracy_score(y_test, y_test_pred) # Calculate Accuracy\n",
        "svm_rbf_test_mcc = matthews_corrcoef(y_test, y_test_pred) # Calculate MCC\n",
        "svm_rbf_test_f1 = f1_score(y_test, y_test_pred, average='weighted') # Calculate F1-score\n",
        "svm_rbf_test_ROC = roc_auc_score(y_test, y_test_pred) # Calculate ROC AUC\n",
        "svm_rbf_test_recall = recall_score(y_test, y_test_pred, average='weighted') # Calculate Recall\n",
        "svm_rbf_test_precision = precision_score(y_test, y_test_pred, average='weighted') # Calculate Precision\n",
        "\n",
        "print('Model performance for Training set')\n",
        "print('- Accuracy: %s' % svm_rbf_train_accuracy)\n",
        "print('- MCC: %s' % svm_rbf_train_mcc)\n",
        "print('- F1 score: %s' % svm_rbf_train_f1)\n",
        "print('----------------------------------')\n",
        "print('Model performance for Test set')\n",
        "print('- Accuracy: %s' % svm_rbf_test_accuracy)\n",
        "print('- MCC: %s' % svm_rbf_test_mcc)\n",
        "print('- F1 score: %s' % svm_rbf_test_f1)\n",
        "print('- ROC AUC: %s' % svm_rbf_test_ROC)\n",
        "print('- Recall: %s' % svm_rbf_test_recall)\n",
        "print('- Precision: %s' % svm_rbf_test_precision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "#save model as pickle file\n",
        "filename = 'svm_rbf_model1hot.sav'\n",
        "pickle.dump(svm_rbf, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tus32H-i42PT"
      },
      "source": [
        "**SVM Polynomial**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3YJF0rz44Ar",
        "outputId": "198405ac-af31-4eb8-d773-5e00b350ad00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model performance for Training set\n",
            "- Accuracy: 1.0\n",
            "- MCC: 1.0\n",
            "- F1 score: 1.0\n",
            "----------------------------------\n",
            "Model performance for Test set\n",
            "- Accuracy: 0.7258064516129032\n",
            "- MCC: 0.5319710492864359\n",
            "- F1 score: 0.7003787984033045\n",
            "- ROC AUC: 0.7166666666666667\n",
            "- Recall: 0.7258064516129032\n",
            "- Precision: 0.8209348255431204\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "poly_kernel_svm_clf = Pipeline((\n",
        "(\"scaler\", StandardScaler()),\n",
        "(\"svm_clf\", SVC(kernel=\"poly\", degree=17, coef0=0, C=1000))\n",
        "))\n",
        "poly_kernel_svm_clf.fit(X_train, y_train) # Train model\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = poly_kernel_svm_clf.predict(X_train)\n",
        "y_test_pred = poly_kernel_svm_clf.predict(X_test)\n",
        "\n",
        "# Training set performance\n",
        "dt_train_accuracy = accuracy_score(y_train, y_train_pred) # Calculate Accuracy\n",
        "dt_train_mcc = matthews_corrcoef(y_train, y_train_pred) # Calculate MCC\n",
        "dt_train_f1 = f1_score(y_train, y_train_pred, average='weighted') # Calculate F1-score\n",
        "\n",
        "# Test set performance\n",
        "dt_test_accuracy = accuracy_score(y_test, y_test_pred) # Calculate Accuracy\n",
        "dt_test_mcc = matthews_corrcoef(y_test, y_test_pred) # Calculate MCC\n",
        "dt_test_f1 = f1_score(y_test, y_test_pred, average='weighted') # Calculate F1-score\n",
        "dt_test_ROC = roc_auc_score(y_test, y_test_pred) # Calculate ROC AUC\n",
        "dt_test_recall = recall_score(y_test, y_test_pred, average='weighted') # Calculate Recall\n",
        "dt_test_precision = precision_score(y_test, y_test_pred, average='weighted') # Calculate Precision\n",
        "\n",
        "print('Model performance for Training set')\n",
        "print('- Accuracy: %s' % dt_train_accuracy)\n",
        "print('- MCC: %s' % dt_train_mcc)\n",
        "print('- F1 score: %s' % dt_train_f1)\n",
        "print('----------------------------------')\n",
        "print('Model performance for Test set')\n",
        "print('- Accuracy: %s' % dt_test_accuracy)\n",
        "print('- MCC: %s' % dt_test_mcc)\n",
        "print('- F1 score: %s' % dt_test_f1)\n",
        "print('- ROC AUC: %s' % dt_test_ROC)\n",
        "print('- Recall: %s' % dt_test_recall)\n",
        "print('- Precision: %s' % dt_test_precision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "#save model as pickle file\n",
        "filename = 'poly_kernel_svm_clf_model1hot.sav'\n",
        "pickle.dump(poly_kernel_svm_clf, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXd2iTxuviDb"
      },
      "source": [
        "**Random forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4iahxJWvhVu",
        "outputId": "e4466663-02b7-4e55-9f55-01f568b79a17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model performance for Training set\n",
            "- Accuracy: 1.0\n",
            "- MCC: 1.0\n",
            "- F1 score: 1.0\n",
            "----------------------------------\n",
            "Model performance for Test set\n",
            "- Accuracy: 0.7580645161290323\n",
            "- MCC: 0.5199835409730609\n",
            "- F1 score: 0.7558332270814738\n",
            "- ROC AUC: 0.7552083333333333\n",
            "- Recall: 0.7580645161290323\n",
            "- Precision: 0.7637314734088928\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=1000) # Define classifier\n",
        "rf.fit(X_train, y_train) # Train model\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = rf.predict(X_train)\n",
        "y_test_pred = rf.predict(X_test)\n",
        "\n",
        "# Training set performance\n",
        "rf_train_accuracy = accuracy_score(y_train, y_train_pred) # Calculate Accuracy\n",
        "rf_train_mcc = matthews_corrcoef(y_train, y_train_pred) # Calculate MCC\n",
        "rf_train_f1 = f1_score(y_train, y_train_pred, average='weighted') # Calculate F1-score\n",
        "\n",
        "# Test set performance\n",
        "rf_test_accuracy = accuracy_score(y_test, y_test_pred) # Calculate Accuracy\n",
        "rf_test_mcc = matthews_corrcoef(y_test, y_test_pred) # Calculate MCC\n",
        "rf_test_f1 = f1_score(y_test, y_test_pred, average='weighted') # Calculate F1-score\n",
        "rf_test_ROC = roc_auc_score(y_test, y_test_pred) # Calculate ROC AUC\n",
        "rf_test_recall = recall_score(y_test, y_test_pred, average='weighted') # Calculate Recall\n",
        "rf_test_precision = precision_score(y_test, y_test_pred, average='weighted') # Calculate Precision\n",
        "\n",
        "print('Model performance for Training set')\n",
        "print('- Accuracy: %s' % rf_train_accuracy)\n",
        "print('- MCC: %s' % rf_train_mcc)\n",
        "print('- F1 score: %s' % rf_train_f1)\n",
        "print('----------------------------------')\n",
        "print('Model performance for Test set')\n",
        "print('- Accuracy: %s' % rf_test_accuracy)\n",
        "print('- MCC: %s' % rf_test_mcc)\n",
        "print('- F1 score: %s' % rf_test_f1)\n",
        "print('- ROC AUC: %s' % rf_test_ROC)\n",
        "print('- Recall: %s' % rf_test_recall)\n",
        "print('- Precision: %s' % rf_test_precision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "#save model as pickle file\n",
        "filename = 'rf_model1hot.sav'\n",
        "pickle.dump(rf, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_H6KkezwfH0"
      },
      "source": [
        "**Multi-Layer Perceptron**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06mNcVuUwrpi",
        "outputId": "f4e49b00-d3ea-4501-8669-cab30141e7f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model performance for Training set\n",
            "- Accuracy: 0.6888489208633094\n",
            "- MCC: 0.37970097077108056\n",
            "- F1 score: 0.688168144559005\n",
            "----------------------------------\n",
            "Model performance for Test set\n",
            "- Accuracy: 0.7258064516129032\n",
            "- MCC: 0.45487796963448873\n",
            "- F1 score: 0.7255924051270429\n",
            "- ROC AUC: 0.7270833333333333\n",
            "- Recall: 0.7258064516129032\n",
            "- Precision: 0.7287895641622005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/brendanwallace-nash/tensorflow-test/env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "mlp = MLPClassifier(alpha=0.1, max_iter=10)\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = mlp.predict(X_train)\n",
        "y_test_pred = mlp.predict(X_test)\n",
        "\n",
        "# Training set performance\n",
        "mlp_train_accuracy = accuracy_score(y_train, y_train_pred) # Calculate Accuracy\n",
        "mlp_train_mcc = matthews_corrcoef(y_train, y_train_pred) # Calculate MCC\n",
        "mlp_train_f1 = f1_score(y_train, y_train_pred, average='weighted') # Calculate F1-score\n",
        "\n",
        "# Test set performance\n",
        "mlp_test_accuracy = accuracy_score(y_test, y_test_pred) # Calculate Accuracy\n",
        "mlp_test_mcc = matthews_corrcoef(y_test, y_test_pred) # Calculate MCC\n",
        "mlp_test_f1 = f1_score(y_test, y_test_pred, average='weighted') # Calculate F1-score\n",
        "mlp_test_ROC = roc_auc_score(y_test, y_test_pred) # Calculate ROC AUC\n",
        "mlp_test_recall = recall_score(y_test, y_test_pred, average='weighted') # Calculate Recall\n",
        "mlp_test_precision = precision_score(y_test, y_test_pred, average='weighted') # Calculate Precision\n",
        "\n",
        "\n",
        "print('Model performance for Training set')\n",
        "print('- Accuracy: %s' % mlp_train_accuracy)\n",
        "print('- MCC: %s' % mlp_train_mcc)\n",
        "print('- F1 score: %s' % mlp_train_f1)\n",
        "print('----------------------------------')\n",
        "print('Model performance for Test set')\n",
        "print('- Accuracy: %s' % mlp_test_accuracy)\n",
        "print('- MCC: %s' % mlp_test_mcc)\n",
        "print('- F1 score: %s' % mlp_test_f1)\n",
        "print('- ROC AUC: %s' % mlp_test_ROC)\n",
        "print('- Recall: %s' % mlp_test_recall)\n",
        "print('- Precision: %s' % mlp_test_precision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "#save model as pickle file\n",
        "filename = 'mlp_model1hot.sav'\n",
        "pickle.dump(mlp, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPP95Rpyt8go"
      },
      "source": [
        "# **Build Stacked model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/brendanwallace-nash/tensorflow-test/env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Users/brendanwallace-nash/tensorflow-test/env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Users/brendanwallace-nash/tensorflow-test/env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Users/brendanwallace-nash/tensorflow-test/env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Users/brendanwallace-nash/tensorflow-test/env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Users/brendanwallace-nash/tensorflow-test/env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model performance for Training set\n",
            "- Accuracy: 1.0\n",
            "- MCC: 1.0\n",
            "- F1 score: 1.0\n",
            "----------------------------------\n",
            "Model performance for Test set\n",
            "- Accuracy: 0.7903225806451613\n",
            "- MCC: 0.6027585316394857\n",
            "- F1 score: 0.7847469522497466\n",
            "- ROC AUC: 0.7854166666666667\n",
            "- Recall: 0.7903225806451613\n",
            "- Precision: 0.8154434078903001\n"
          ]
        }
      ],
      "source": [
        "# Define estimators\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "estimator_list = [\n",
        "    ('knn',knn),\n",
        "    ('svm_rbf',svm_rbf),\n",
        "    ('poly_kernel_svm_clf',poly_kernel_svm_clf),\n",
        "    ('rf',rf),\n",
        "    ('mlp',mlp), ]\n",
        "\n",
        "# Build stack model\n",
        "stack_model = StackingClassifier(\n",
        "    estimators=estimator_list, final_estimator=LogisticRegression()\n",
        ")\n",
        "\n",
        "# Train stacked model\n",
        "stack_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = stack_model.predict(X_train)\n",
        "y_test_pred = stack_model.predict(X_test)\n",
        "\n",
        "# Training set model performance\n",
        "stack_model_train_accuracy = accuracy_score(y_train, y_train_pred) # Calculate Accuracy\n",
        "stack_model_train_mcc = matthews_corrcoef(y_train, y_train_pred) # Calculate MCC\n",
        "stack_model_train_f1 = f1_score(y_train, y_train_pred, average='weighted') # Calculate F1-score\n",
        "\n",
        "# Test set model performance\n",
        "stack_model_test_accuracy = accuracy_score(y_test, y_test_pred) # Calculate Accuracy\n",
        "stack_model_test_mcc = matthews_corrcoef(y_test, y_test_pred) # Calculate MCC\n",
        "stack_model_test_f1 = f1_score(y_test, y_test_pred, average='weighted') # Calculate F1-score\n",
        "stack_model_test_ROC = roc_auc_score(y_test, y_test_pred) # Calculate ROC AUC\n",
        "stack_model_test_recall = recall_score(y_test, y_test_pred, average='weighted') # Calculate Recall\n",
        "stack_model_test_precision = precision_score(y_test, y_test_pred, average='weighted') # Calculate Precision\n",
        "\n",
        "print('Model performance for Training set')\n",
        "print('- Accuracy: %s' % stack_model_train_accuracy)\n",
        "print('- MCC: %s' % stack_model_train_mcc)\n",
        "print('- F1 score: %s' % stack_model_train_f1)\n",
        "print('----------------------------------')\n",
        "print('Model performance for Test set')\n",
        "print('- Accuracy: %s' % stack_model_test_accuracy)\n",
        "print('- MCC: %s' % stack_model_test_mcc)\n",
        "print('- F1 score: %s' % stack_model_test_f1)\n",
        "print('- ROC AUC: %s' % stack_model_test_ROC)\n",
        "print('- Recall: %s' % stack_model_test_recall)\n",
        "print('- Precision: %s' % stack_model_test_precision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "#save model as pickle file\n",
        "filename = 'stack_model1hot.sav'\n",
        "pickle.dump(stack_model, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model performance for Training set\n",
            "- Accuracy: 1.0\n",
            "- MCC: 1.0\n",
            "- F1 score: 1.0\n",
            "----------------------------------\n",
            "Model performance for Test set\n",
            "- Accuracy: 0.7580645161290323\n",
            "- MCC: 0.5345632233153104\n",
            "- F1 score: 0.7516310987497076\n",
            "- ROC AUC: 0.753125\n",
            "- Recall: 0.7580645161290323\n",
            "- Precision: 0.7798134202540182\n"
          ]
        }
      ],
      "source": [
        "# Define estimators\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "estimator_list = [\n",
        "    ('knn',knn),\n",
        "    ('poly_kernel_svm_clf',poly_kernel_svm_clf),\n",
        "    ('rf',rf) ]\n",
        "\n",
        "# Build stack model\n",
        "stack_model = StackingClassifier(\n",
        "    estimators=estimator_list, final_estimator=LogisticRegression()\n",
        ")\n",
        "\n",
        "# Train stacked model\n",
        "stack_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = stack_model.predict(X_train)\n",
        "y_test_pred = stack_model.predict(X_test)\n",
        "\n",
        "# Training set model performance\n",
        "stack_model_train_accuracy = accuracy_score(y_train, y_train_pred) # Calculate Accuracy\n",
        "stack_model_train_mcc = matthews_corrcoef(y_train, y_train_pred) # Calculate MCC\n",
        "stack_model_train_f1 = f1_score(y_train, y_train_pred, average='weighted') # Calculate F1-score\n",
        "\n",
        "# Test set model performance\n",
        "stack_model_test_accuracy = accuracy_score(y_test, y_test_pred) # Calculate Accuracy\n",
        "stack_model_test_mcc = matthews_corrcoef(y_test, y_test_pred) # Calculate MCC\n",
        "stack_model_test_f1 = f1_score(y_test, y_test_pred, average='weighted') # Calculate F1-score\n",
        "stack_model_test_ROC = roc_auc_score(y_test, y_test_pred) # Calculate ROC AUC\n",
        "stack_model_test_recall = recall_score(y_test, y_test_pred, average='weighted') # Calculate Recall\n",
        "stack_model_test_precision = precision_score(y_test, y_test_pred, average='weighted') # Calculate Precision\n",
        "\n",
        "print('Model performance for Training set')\n",
        "print('- Accuracy: %s' % stack_model_train_accuracy)\n",
        "print('- MCC: %s' % stack_model_train_mcc)\n",
        "print('- F1 score: %s' % stack_model_train_f1)\n",
        "print('----------------------------------')\n",
        "print('Model performance for Test set')\n",
        "print('- Accuracy: %s' % stack_model_test_accuracy)\n",
        "print('- MCC: %s' % stack_model_test_mcc)\n",
        "print('- F1 score: %s' % stack_model_test_f1)\n",
        "print('- ROC AUC: %s' % stack_model_test_ROC)\n",
        "print('- Recall: %s' % stack_model_test_recall)\n",
        "print('- Precision: %s' % stack_model_test_precision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "#save model as pickle file\n",
        "filename = 'stack_model1hot2.sav'\n",
        "pickle.dump(stack_model, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WO_qR3303OUp",
        "outputId": "985e12c1-9e17-4bd1-f9ca-8d705b47afd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model performance for Training set\n",
            "- Accuracy: 1.0\n",
            "- MCC: 1.0\n",
            "- F1 score: 1.0\n",
            "----------------------------------\n",
            "Model performance for Test set\n",
            "- Accuracy: 0.7741935483870968\n",
            "- MCC: 0.5746116701117124\n",
            "- F1 score: 0.766870095902354\n",
            "- ROC AUC: 0.76875\n",
            "- Recall: 0.7741935483870968\n",
            "- Precision: 0.804147465437788\n"
          ]
        }
      ],
      "source": [
        "# Define estimators\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "estimator_list = [\n",
        "    ('knn',knn),\n",
        "    ('svm_rbf',svm_rbf),\n",
        "    ('poly_kernel_svm_clf',poly_kernel_svm_clf),\n",
        "    ('rf',rf) ]\n",
        "\n",
        "# Build stack model\n",
        "stack_model = StackingClassifier(\n",
        "    estimators=estimator_list, final_estimator=LogisticRegression()\n",
        ")\n",
        "\n",
        "# Train stacked model\n",
        "stack_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = stack_model.predict(X_train)\n",
        "y_test_pred = stack_model.predict(X_test)\n",
        "\n",
        "# Training set model performance\n",
        "stack_model_train_accuracy = accuracy_score(y_train, y_train_pred) # Calculate Accuracy\n",
        "stack_model_train_mcc = matthews_corrcoef(y_train, y_train_pred) # Calculate MCC\n",
        "stack_model_train_f1 = f1_score(y_train, y_train_pred, average='weighted') # Calculate F1-score\n",
        "\n",
        "# Test set model performance\n",
        "stack_model_test_accuracy = accuracy_score(y_test, y_test_pred) # Calculate Accuracy\n",
        "stack_model_test_mcc = matthews_corrcoef(y_test, y_test_pred) # Calculate MCC\n",
        "stack_model_test_f1 = f1_score(y_test, y_test_pred, average='weighted') # Calculate F1-score\n",
        "stack_model_test_ROC = roc_auc_score(y_test, y_test_pred) # Calculate ROC AUC\n",
        "stack_model_test_recall = recall_score(y_test, y_test_pred, average='weighted') # Calculate Recall\n",
        "stack_model_test_precision = precision_score(y_test, y_test_pred, average='weighted') # Calculate Precision\n",
        "\n",
        "print('Model performance for Training set')\n",
        "print('- Accuracy: %s' % stack_model_train_accuracy)\n",
        "print('- MCC: %s' % stack_model_train_mcc)\n",
        "print('- F1 score: %s' % stack_model_train_f1)\n",
        "print('----------------------------------')\n",
        "print('Model performance for Test set')\n",
        "print('- Accuracy: %s' % stack_model_test_accuracy)\n",
        "print('- MCC: %s' % stack_model_test_mcc)\n",
        "print('- F1 score: %s' % stack_model_test_f1)\n",
        "print('- ROC AUC: %s' % stack_model_test_ROC)\n",
        "print('- Recall: %s' % stack_model_test_recall)\n",
        "print('- Precision: %s' % stack_model_test_precision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#save file as pickle file\n",
        "filename = 'stack_model1hot3.sav'\n",
        "pickle.dump(stack_model, open(filename, 'wb'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Stacking-Classifier.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.9",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "ce0fc99d3feae300fa692521e710fc8ac7d999bfa22db95d812a9b4071499cf3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
