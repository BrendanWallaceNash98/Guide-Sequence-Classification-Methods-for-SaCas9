{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Criteria Algorithms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "X_test = []\n",
    "with (open(\"X_test\", \"rb\")) as openfile:\n",
    "    while True:\n",
    "        try:\n",
    "            X_test = pickle.load(openfile)\n",
    "        except EOFError:\n",
    "            break\n",
    "y_test = []\n",
    "with (open(\"y_test\", \"rb\")) as openfile:\n",
    "    while True:\n",
    "        try:\n",
    "            y_test = pickle.load(openfile)\n",
    "        except EOFError:\n",
    "            break\n",
    "X_train = []\n",
    "with (open(\"X_train\", \"rb\")) as openfile:\n",
    "    while True:\n",
    "        try:\n",
    "            X_train = pickle.load(openfile)\n",
    "        except EOFError:\n",
    "            break\n",
    "y_train = []\n",
    "with (open(\"y_train\", \"rb\")) as openfile:\n",
    "    while True:\n",
    "        try:\n",
    "            y_train = pickle.load(openfile)\n",
    "        except EOFError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodingReverse(guideList):\n",
    "    #this function encodes the guide sequence from numbers back to character\n",
    "    guideSeqList = []\n",
    "    #loop through guide then loop through guide nueclotides to encode back to characters\n",
    "    for guide in guideList:\n",
    "        guideSeq = []\n",
    "        for i in range(len(guide)):\n",
    "            if guide[i] == 1:\n",
    "                guideSeq.append('A')\n",
    "            elif guide[i] == 2:\n",
    "                guideSeq.append('C')\n",
    "            elif guide[i] == 3:\n",
    "                guideSeq.append('G')\n",
    "            elif guide[i] == 4:\n",
    "                guideSeq.append('T')\n",
    "            else:\n",
    "                pass\n",
    "        guideSeqList.append(guideSeq)\n",
    "        #return the encoded guide sequence\n",
    "    return guideSeqList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only encode X_test as that is all that will be used for criteria algorithms \n",
    "X_test = encodingReverse(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def characterCriteria(guideList):\n",
    "    #this function checks for a G nueclotide at the 0th position and append 0 to a list if absent and 1 to a list if present\n",
    "    #this formating matches that of the X_train and X_test data\n",
    "    resultList = []\n",
    "    for guide in guideList:\n",
    "        if guide[0] == 'G':\n",
    "            resultList.append(1)\n",
    "        else:\n",
    "            resultList.append(0)\n",
    "    return resultList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only the test data is used to be consistent with the testing of mahine learning models\n",
    "X_testResults = characterCriteria(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score:  0.5641754636833916\n",
      "Recall:  0.5645161290322581\n",
      "Precision:  0.5641284929382816\n"
     ]
    }
   ],
   "source": [
    "# Test set performance\n",
    "#Skleanr metric packages are installed\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "#Calculations for F1, recall and precision\n",
    "characterCriteriaF1 = f1_score(y_test, X_testResults, average='weighted') # Calculate F1-score\n",
    "characterCriteriaRecall = recall_score(y_test, X_testResults, average='weighted') # Calculate Recall\n",
    "characterCriteriaRecallPercision = precision_score(y_test, X_testResults, average='weighted') # Calculate Precision\n",
    "\n",
    "print('F1-score: ', characterCriteriaF1)\n",
    "print('Recall: ', characterCriteriaRecall)\n",
    "print('Precision: ', characterCriteriaRecallPercision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the criteria algorithim for effecient guide lenghts\n",
    "def lengthCriteria(guideList):\n",
    "    #this function checks for a guide length of 20 and append 0 to a list if absent and 1 to a list if present\n",
    "    #this formating matches that of the X_train and X_test data\n",
    "    resultList = []\n",
    "    for guide in guideList:\n",
    "        if len(guide) == 21:\n",
    "            resultList.append(1)\n",
    "        elif len(guide) == 22:\n",
    "            resultList.append(1)\n",
    "        else:\n",
    "            resultList.append(0)\n",
    "    return resultList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only the test data is used to be consistent with the testing of mahine learning models\n",
    "X_testResults = lengthCriteria(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score:  0.5860518312495125\n",
      "Recall:  0.5967741935483871\n",
      "Precision:  0.6016634820726088\n"
     ]
    }
   ],
   "source": [
    "#Calculations for F1, recall and precision\n",
    "LenghtCriteriaF1 = f1_score(y_test, X_testResults, average='weighted') # Calculate F1-score\n",
    "LenghtCriteriaRecall = recall_score(y_test, X_testResults, average='weighted') # Calculate Recall\n",
    "LenghtCriteriaRecallPercision = precision_score(y_test, X_testResults, average='weighted') # Calculate Precision\n",
    "\n",
    "print('F1-score: ', LenghtCriteriaF1)\n",
    "print('Recall: ', LenghtCriteriaRecall)\n",
    "print('Precision: ', LenghtCriteriaRecallPercision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce0fc99d3feae300fa692521e710fc8ac7d999bfa22db95d812a9b4071499cf3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
