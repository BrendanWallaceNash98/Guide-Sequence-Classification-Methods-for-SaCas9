{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "escsGkWFtzVj"
      },
      "source": [
        "# **2-Mer Classification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "g-NFn_UOtuVE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "X_test = []\n",
        "with (open(\"X_test\", \"rb\")) as openfile:\n",
        "    while True:\n",
        "        try:\n",
        "            X_test = pickle.load(openfile)\n",
        "        except EOFError:\n",
        "            break\n",
        "y_test = []\n",
        "with (open(\"y_test\", \"rb\")) as openfile:\n",
        "    while True:\n",
        "        try:\n",
        "            y_test = pickle.load(openfile)\n",
        "        except EOFError:\n",
        "            break\n",
        "X_train = []\n",
        "with (open(\"X_train\", \"rb\")) as openfile:\n",
        "    while True:\n",
        "        try:\n",
        "            X_train = pickle.load(openfile)\n",
        "        except EOFError:\n",
        "            break\n",
        "y_train = []\n",
        "with (open(\"y_train\", \"rb\")) as openfile:\n",
        "    while True:\n",
        "        try:\n",
        "            y_train = pickle.load(openfile)\n",
        "        except EOFError:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encodingReverse(guide):\n",
        "    #this function encodes the guide sequence into a 4x4 matrix\n",
        "    #varaible decleration\n",
        "    guideSeq = []\n",
        "    #for loop to encode the guide sequence\n",
        "    for i in range(len(guide)):\n",
        "        if guide[i] == 1:\n",
        "            guideSeq.append('A')\n",
        "        elif guide[i] == 2:\n",
        "            guideSeq.append('C')\n",
        "        elif guide[i] == 3:\n",
        "            guideSeq.append('G')\n",
        "        elif guide[i] == 4:\n",
        "            guideSeq.append('T')\n",
        "        else:\n",
        "            pass\n",
        "    #return the encoded guide sequence\n",
        "    return guideSeq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(len(X_train)):\n",
        "    X_train[i] = encodingReverse(X_train[i])\n",
        "for i in range(len(X_test)):\n",
        "    X_test[i] = encodingReverse(X_test[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_kmers(read, k):\n",
        "    \"\"\"Count kmer occurrences in a given read.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    read : string\n",
        "        A single DNA sequence.\n",
        "    k : int\n",
        "        The value of k for which to count kmers.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    counts : dictionary, {'string': int}\n",
        "        A dictionary of counts keyed by their individual kmers (strings\n",
        "        of length k).\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> count_kmers(\"GATGAT\", 3)\n",
        "    {'ATG': 1, 'GAT': 2, 'TGA': 1}\n",
        "    \"\"\"\n",
        "    # Start with an empty dictionary\n",
        "    counts = {}\n",
        "    # Calculate how many kmers of length k there are\n",
        "    num_kmers = len(read) - k + 1\n",
        "    # Loop over the kmer start positions\n",
        "    for i in range(num_kmers):\n",
        "        # Slice the string to get the kmer\n",
        "        kmer = read[i:i+k]\n",
        "        # Add the kmer to the dictionary if it's not there\n",
        "        if kmer not in counts:\n",
        "            counts[kmer] = 0\n",
        "        # Increment the count for this kmer\n",
        "        counts[kmer] += 1\n",
        "    # Return the final counts\n",
        "    return counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Emx1 genome \n",
        "EMX1_GENOME = r\"/Users/brendanwallace-nash/Library/CloudStorage/OneDrive-QueenslandUniversityofTechnology/Research Project-Brendanâ€™s MacBook Pro/Guide Sequence/EMX1.fna\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#get the genome string for EMX1, this will be used to get all 64 lenght 3 kmers\n",
        "seq = \"\"\n",
        "guideSeq = {}\n",
        "with open(EMX1_GENOME, 'r') as fp:\n",
        "    for line in fp:\n",
        "        if not line[0] == \">\":\n",
        "            seq += line.rstrip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "#this is to get a dictionary of all 64 kmers in a dictioanry and set the value to 0\n",
        "kmerDic = count_kmers(seq, 2)\n",
        "for key in kmerDic:\n",
        "    kmerDic[key] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getKmers(guideList, empKemerDic):\n",
        "    #this function takes in a list of guide sequences and a dictionary of all 64 kmers and returns a list of dictionaries\n",
        "    #variable decleration\n",
        "    guideKmerList = []\n",
        "    finalKmerList = []\n",
        "    #for loop to get the kmers for each guide sequence\n",
        "    for i in range(len(guideList)):\n",
        "        string = \"\"\n",
        "        for letter in X_train[i]:\n",
        "            string = string + letter\n",
        "        string = string.upper()\n",
        "        guideKmerList.append(count_kmers(string, 2))\n",
        "    #for loop to add the kmers to the dictionary\n",
        "    for i in range(len(guideKmerList)):\n",
        "        tempKmerDic = empKemerDic.copy()\n",
        "        for key in guideKmerList[i]:\n",
        "            tempKmerDic[key] += guideKmerList[i][key]\n",
        "        finalKmerList.append(tempKmerDic)\n",
        "    #return the dictionary\n",
        "    return finalKmerList"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Convert the guide sequences into a list of dictionaries\n",
        "trainKmers = getKmers(X_train, kmerDic)\n",
        "testKmers = getKmers(X_test, kmerDic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainKmersDF = pd.DataFrame(trainKmers)\n",
        "testKmersDF = pd.DataFrame(testKmers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>GA</th>\n",
              "      <th>AG</th>\n",
              "      <th>GC</th>\n",
              "      <th>CG</th>\n",
              "      <th>CT</th>\n",
              "      <th>TC</th>\n",
              "      <th>CC</th>\n",
              "      <th>CA</th>\n",
              "      <th>AA</th>\n",
              "      <th>AC</th>\n",
              "      <th>GG</th>\n",
              "      <th>TG</th>\n",
              "      <th>GT</th>\n",
              "      <th>TT</th>\n",
              "      <th>AT</th>\n",
              "      <th>TA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>62 rows Ã— 16 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    GA  AG  GC  CG  CT  TC  CC  CA  AA  AC  GG  TG  GT  TT  AT  TA\n",
              "0    0   0   1   0   2   1   4   3   0   4   2   1   0   1   0   0\n",
              "1    1   1   0   0   1   2   3   2   0   1   2   1   1   3   1   0\n",
              "2    1   2   1   0   1   2   1   2   1   1   1   1   1   1   2   1\n",
              "3    2   2   2   2   0   0   0   0   0   0   2   5   5   0   0   0\n",
              "4    2   0   0   1   1   3   2   2   1   2   0   2   1   0   2   0\n",
              "..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..\n",
              "57   0   0   2   1   1   1   2   1   2   0   2   2   0   0   3   1\n",
              "58   2   2   3   3   0   0   2   2   1   2   2   1   0   0   1   0\n",
              "59   0   1   3   1   2   1   5   1   1   0   3   0   0   1   0   0\n",
              "60   1   0   1   1   2   2   1   1   1   2   2   2   1   1   0   0\n",
              "61   1   2   0   0   0   1   2   1   1   0   3   3   4   1   0   0\n",
              "\n",
              "[62 rows x 16 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "testKmersDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dfValueToList(df):\n",
        "    #this function takes in a dataframe and returns a list \n",
        "    #variable decleration\n",
        "    kmerList = []\n",
        "    #for loop to convert the dataframe to a list \n",
        "    for i in range(len(df)):\n",
        "            kmerList.append(list(df.iloc[i]))\n",
        "    #return the list\n",
        "    return kmerList"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = dfValueToList(trainKmersDF)\n",
        "X_test = dfValueToList(testKmersDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UpZfDRyvb5t"
      },
      "source": [
        "# **Build Classification models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Q8yCRtbQu5-F"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LswMOe9Y26Nm"
      },
      "source": [
        "**K nearest neighbors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fI6Ni6i3EAy",
        "outputId": "fcd88810-42d7-41d6-d4c4-90d808704f14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model performance for Training set\n",
            "- Accuracy: 0.8597122302158273\n",
            "- MCC: 0.7196899065776223\n",
            "- F1 score: 0.8596759139886475\n",
            "----------------------------------\n",
            "Model performance for Test set\n",
            "- Accuracy: 0.6451612903225806\n",
            "- MCC: 0.28870545602072445\n",
            "- F1 score: 0.6444197256210604\n",
            "- ROC AUC: 0.64375\n",
            "- Recall: 0.6451612903225806\n",
            "- Precision: 0.6450257522363785\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=3) # Define classifier\n",
        "knn.fit(X_train, y_train) # Train model\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = knn.predict(X_train)\n",
        "y_test_pred = knn.predict(X_test)\n",
        "\n",
        "# Training set performance\n",
        "knn_train_accuracy = accuracy_score(y_train, y_train_pred) # Calculate Accuracy\n",
        "knn_train_mcc = matthews_corrcoef(y_train, y_train_pred) # Calculate MCC\n",
        "knn_train_f1 = f1_score(y_train, y_train_pred, average='weighted') # Calculate F1-score\n",
        "\n",
        "# Test set performance\n",
        "knn_test_accuracy = accuracy_score(y_test, y_test_pred) # Calculate Accuracy\n",
        "knn_test_mcc = matthews_corrcoef(y_test, y_test_pred) # Calculate MCC\n",
        "knn_test_f1 = f1_score(y_test, y_test_pred, average='weighted') # Calculate F1-score\n",
        "knn_test_ROC = roc_auc_score(y_test, y_test_pred) # Calculate ROC AUC\n",
        "knn_test_recall = recall_score(y_test, y_test_pred, average='weighted') # Calculate Recall\n",
        "knn_test_precision = precision_score(y_test, y_test_pred, average='weighted') # Calculate Precision\n",
        "\n",
        "print('Model performance for Training set')\n",
        "print('- Accuracy: %s' % knn_train_accuracy)\n",
        "print('- MCC: %s' % knn_train_mcc)\n",
        "print('- F1 score: %s' % knn_train_f1)\n",
        "print('----------------------------------')\n",
        "print('Model performance for Test set')\n",
        "print('- Accuracy: %s' % knn_test_accuracy)\n",
        "print('- MCC: %s' % knn_test_mcc)\n",
        "print('- F1 score: %s' % knn_test_f1)\n",
        "print('- ROC AUC: %s' % knn_test_ROC)\n",
        "print('- Recall: %s' % knn_test_recall)\n",
        "print('- Precision: %s' % knn_test_precision)\n",
        "\n",
        "#Save model as pickle file\n",
        "filename = 'knn_2Mer.sav'\n",
        "pickle.dump(knn, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojasbTOn4-x-"
      },
      "source": [
        "**Support vector machine (Radial basis function kernel)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ot6hHeU04-2j",
        "outputId": "b95d2e3a-5cd2-47c7-cdc7-87bbd8b9da52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model performance for Training set\n",
            "- Accuracy: 1.0\n",
            "- MCC: 1.0\n",
            "- F1 score: 1.0\n",
            "----------------------------------\n",
            "Model performance for Test set\n",
            "- Accuracy: 0.5645161290322581\n",
            "- MCC: 0.1272823676500175\n",
            "- F1 score: 0.5641754636833916\n",
            "- ROC AUC: 0.5635416666666666\n",
            "- Recall: 0.5645161290322581\n",
            "- Precision: 0.5641284929382816\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm_rbf = SVC(gamma=2, C=1)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = svm_rbf.predict(X_train)\n",
        "y_test_pred = svm_rbf.predict(X_test)\n",
        "\n",
        "# Training set performance\n",
        "svm_rbf_train_accuracy = accuracy_score(y_train, y_train_pred) # Calculate Accuracy\n",
        "svm_rbf_train_mcc = matthews_corrcoef(y_train, y_train_pred) # Calculate MCC\n",
        "svm_rbf_train_f1 = f1_score(y_train, y_train_pred, average='weighted') # Calculate F1-score\n",
        "\n",
        "# Test set performance\n",
        "svm_rbf_test_accuracy = accuracy_score(y_test, y_test_pred) # Calculate Accuracy\n",
        "svm_rbf_test_mcc = matthews_corrcoef(y_test, y_test_pred) # Calculate MCC\n",
        "svm_rbf_test_f1 = f1_score(y_test, y_test_pred, average='weighted') # Calculate F1-score\n",
        "svm_rbf_test_ROC = roc_auc_score(y_test, y_test_pred) # Calculate ROC AUC\n",
        "svm_rbf_test_recall = recall_score(y_test, y_test_pred, average='weighted') # Calculate Recall\n",
        "svm_rbf_test_precision = precision_score(y_test, y_test_pred, average='weighted') # Calculate Precision\n",
        "\n",
        "print('Model performance for Training set')\n",
        "print('- Accuracy: %s' % svm_rbf_train_accuracy)\n",
        "print('- MCC: %s' % svm_rbf_train_mcc)\n",
        "print('- F1 score: %s' % svm_rbf_train_f1)\n",
        "print('----------------------------------')\n",
        "print('Model performance for Test set')\n",
        "print('- Accuracy: %s' % svm_rbf_test_accuracy)\n",
        "print('- MCC: %s' % svm_rbf_test_mcc)\n",
        "print('- F1 score: %s' % svm_rbf_test_f1)\n",
        "print('- ROC AUC: %s' % svm_rbf_test_ROC)\n",
        "print('- Recall: %s' % svm_rbf_test_recall)\n",
        "print('- Precision: %s' % svm_rbf_test_precision)\n",
        "\n",
        "#Save model as pickle file\n",
        "filename = 'svm_rbf_2Mer.sav'\n",
        "pickle.dump(svm_rbf, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tus32H-i42PT"
      },
      "source": [
        "**SVM Polynomial**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3YJF0rz44Ar",
        "outputId": "198405ac-af31-4eb8-d773-5e00b350ad00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model performance for Training set\n",
            "- Accuracy: 0.9226618705035972\n",
            "- MCC: 0.8555437243309014\n",
            "- F1 score: 0.9221745897465564\n",
            "----------------------------------\n",
            "Model performance for Test set\n",
            "- Accuracy: 0.5645161290322581\n",
            "- MCC: 0.13438736818105856\n",
            "- F1 score: 0.5628128022879254\n",
            "- ROC AUC: 0.5666666666666667\n",
            "- Recall: 0.5645161290322581\n",
            "- Precision: 0.5685270523980201\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "poly_kernel_svm_clf = Pipeline((\n",
        "(\"scaler\", StandardScaler()),\n",
        "(\"svm_clf\", SVC(kernel=\"poly\", degree=17, coef0=0, C=1000))\n",
        "))\n",
        "poly_kernel_svm_clf.fit(X_train, y_train) # Train model\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = poly_kernel_svm_clf.predict(X_train)\n",
        "y_test_pred = poly_kernel_svm_clf.predict(X_test)\n",
        "\n",
        "# Training set performance\n",
        "dt_train_accuracy = accuracy_score(y_train, y_train_pred) # Calculate Accuracy\n",
        "dt_train_mcc = matthews_corrcoef(y_train, y_train_pred) # Calculate MCC\n",
        "dt_train_f1 = f1_score(y_train, y_train_pred, average='weighted') # Calculate F1-score\n",
        "\n",
        "# Test set performance\n",
        "dt_test_accuracy = accuracy_score(y_test, y_test_pred) # Calculate Accuracy\n",
        "dt_test_mcc = matthews_corrcoef(y_test, y_test_pred) # Calculate MCC\n",
        "dt_test_f1 = f1_score(y_test, y_test_pred, average='weighted') # Calculate F1-score\n",
        "dt_test_ROC = roc_auc_score(y_test, y_test_pred) # Calculate ROC AUC\n",
        "dt_test_recall = recall_score(y_test, y_test_pred, average='weighted') # Calculate Recall\n",
        "dt_test_precision = precision_score(y_test, y_test_pred, average='weighted') # Calculate Precision\n",
        "\n",
        "print('Model performance for Training set')\n",
        "print('- Accuracy: %s' % dt_train_accuracy)\n",
        "print('- MCC: %s' % dt_train_mcc)\n",
        "print('- F1 score: %s' % dt_train_f1)\n",
        "print('----------------------------------')\n",
        "print('Model performance for Test set')\n",
        "print('- Accuracy: %s' % dt_test_accuracy)\n",
        "print('- MCC: %s' % dt_test_mcc)\n",
        "print('- F1 score: %s' % dt_test_f1)\n",
        "print('- ROC AUC: %s' % dt_test_ROC)\n",
        "print('- Recall: %s' % dt_test_recall)\n",
        "print('- Precision: %s' % dt_test_precision)\n",
        "\n",
        "#Save model as pickle file\n",
        "filename = 'poly_kernel_svm_clf_2Mer.sav'\n",
        "pickle.dump(poly_kernel_svm_clf, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXd2iTxuviDb"
      },
      "source": [
        "**Random forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4iahxJWvhVu",
        "outputId": "e4466663-02b7-4e55-9f55-01f568b79a17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model performance for Training set\n",
            "- Accuracy: 1.0\n",
            "- MCC: 1.0\n",
            "- F1 score: 1.0\n",
            "----------------------------------\n",
            "Model performance for Test set\n",
            "- Accuracy: 0.5645161290322581\n",
            "- MCC: 0.1272823676500175\n",
            "- F1 score: 0.5641754636833916\n",
            "- ROC AUC: 0.5635416666666666\n",
            "- Recall: 0.5645161290322581\n",
            "- Precision: 0.5641284929382816\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=1000) # Define classifier\n",
        "rf.fit(X_train, y_train) # Train model\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = rf.predict(X_train)\n",
        "y_test_pred = rf.predict(X_test)\n",
        "\n",
        "# Training set performance\n",
        "rf_train_accuracy = accuracy_score(y_train, y_train_pred) # Calculate Accuracy\n",
        "rf_train_mcc = matthews_corrcoef(y_train, y_train_pred) # Calculate MCC\n",
        "rf_train_f1 = f1_score(y_train, y_train_pred, average='weighted') # Calculate F1-score\n",
        "\n",
        "# Test set performance\n",
        "rf_test_accuracy = accuracy_score(y_test, y_test_pred) # Calculate Accuracy\n",
        "rf_test_mcc = matthews_corrcoef(y_test, y_test_pred) # Calculate MCC\n",
        "rf_test_f1 = f1_score(y_test, y_test_pred, average='weighted') # Calculate F1-score\n",
        "rf_test_ROC = roc_auc_score(y_test, y_test_pred) # Calculate ROC AUC\n",
        "rf_test_recall = recall_score(y_test, y_test_pred, average='weighted') # Calculate Recall\n",
        "rf_test_precision = precision_score(y_test, y_test_pred, average='weighted') # Calculate Precision\n",
        "\n",
        "print('Model performance for Training set')\n",
        "print('- Accuracy: %s' % rf_train_accuracy)\n",
        "print('- MCC: %s' % rf_train_mcc)\n",
        "print('- F1 score: %s' % rf_train_f1)\n",
        "print('----------------------------------')\n",
        "print('Model performance for Test set')\n",
        "print('- Accuracy: %s' % rf_test_accuracy)\n",
        "print('- MCC: %s' % rf_test_mcc)\n",
        "print('- F1 score: %s' % rf_test_f1)\n",
        "print('- ROC AUC: %s' % rf_test_ROC)\n",
        "print('- Recall: %s' % rf_test_recall)\n",
        "print('- Precision: %s' % rf_test_precision)\n",
        "\n",
        "#Save model as pickle file\n",
        "filename = 'rf_2Mer.sav'\n",
        "pickle.dump(rf, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_H6KkezwfH0"
      },
      "source": [
        "**Multi-Layer Perceptron**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06mNcVuUwrpi",
        "outputId": "f4e49b00-d3ea-4501-8669-cab30141e7f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model performance for Training set\n",
            "- Accuracy: 0.6223021582733813\n",
            "- MCC: 0.2445945421373394\n",
            "- F1 score: 0.6223021582733813\n",
            "----------------------------------\n",
            "Model performance for Test set\n",
            "- Accuracy: 0.6290322580645161\n",
            "- MCC: 0.26082452387298666\n",
            "- F1 score: 0.6287426657601168\n",
            "- ROC AUC: 0.6302083333333333\n",
            "- Recall: 0.6290322580645161\n",
            "- Precision: 0.631408635858024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/brendanwallace-nash/tensorflow-test/env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "mlp = MLPClassifier(alpha=0.01, max_iter=10)\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = mlp.predict(X_train)\n",
        "y_test_pred = mlp.predict(X_test)\n",
        "\n",
        "# Training set performance\n",
        "mlp_train_accuracy = accuracy_score(y_train, y_train_pred) # Calculate Accuracy\n",
        "mlp_train_mcc = matthews_corrcoef(y_train, y_train_pred) # Calculate MCC\n",
        "mlp_train_f1 = f1_score(y_train, y_train_pred, average='weighted') # Calculate F1-score\n",
        "\n",
        "# Test set performance\n",
        "mlp_test_accuracy = accuracy_score(y_test, y_test_pred) # Calculate Accuracy\n",
        "mlp_test_mcc = matthews_corrcoef(y_test, y_test_pred) # Calculate MCC\n",
        "mlp_test_f1 = f1_score(y_test, y_test_pred, average='weighted') # Calculate F1-score\n",
        "mlp_test_ROC = roc_auc_score(y_test, y_test_pred) # Calculate ROC AUC\n",
        "mlp_test_recall = recall_score(y_test, y_test_pred, average='weighted') # Calculate Recall\n",
        "mlp_test_precision = precision_score(y_test, y_test_pred, average='weighted') # Calculate Precision\n",
        "\n",
        "\n",
        "print('Model performance for Training set')\n",
        "print('- Accuracy: %s' % mlp_train_accuracy)\n",
        "print('- MCC: %s' % mlp_train_mcc)\n",
        "print('- F1 score: %s' % mlp_train_f1)\n",
        "print('----------------------------------')\n",
        "print('Model performance for Test set')\n",
        "print('- Accuracy: %s' % mlp_test_accuracy)\n",
        "print('- MCC: %s' % mlp_test_mcc)\n",
        "print('- F1 score: %s' % mlp_test_f1)\n",
        "print('- ROC AUC: %s' % mlp_test_ROC)\n",
        "print('- Recall: %s' % mlp_test_recall)\n",
        "print('- Precision: %s' % mlp_test_precision)\n",
        "\n",
        "#Save model as pickle file\n",
        "filename = 'mlp_2Mer.sav'\n",
        "pickle.dump(mlp, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPP95Rpyt8go"
      },
      "source": [
        "# **Build Stacked model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/brendanwallace-nash/tensorflow-test/env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Users/brendanwallace-nash/tensorflow-test/env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Users/brendanwallace-nash/tensorflow-test/env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Users/brendanwallace-nash/tensorflow-test/env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Users/brendanwallace-nash/tensorflow-test/env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Users/brendanwallace-nash/tensorflow-test/env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model performance for Training set\n",
            "- Accuracy: 1.0\n",
            "- MCC: 1.0\n",
            "- F1 score: 1.0\n",
            "----------------------------------\n",
            "Model performance for Test set\n",
            "- Accuracy: 0.5645161290322581\n",
            "- MCC: 0.1272823676500175\n",
            "- F1 score: 0.5641754636833916\n",
            "- ROC AUC: 0.5635416666666666\n",
            "- Recall: 0.5645161290322581\n",
            "- Precision: 0.5641284929382816\n"
          ]
        }
      ],
      "source": [
        "# Define estimators\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "estimator_list = [\n",
        "    ('knn',knn),\n",
        "    ('svm_rbf',svm_rbf),\n",
        "    ('poly_kernel_svm_clf',poly_kernel_svm_clf),\n",
        "    ('rf',rf),\n",
        "    ('mlp',mlp), ]\n",
        "\n",
        "# Build stack model\n",
        "stack_model = StackingClassifier(\n",
        "    estimators=estimator_list, final_estimator=LogisticRegression()\n",
        ")\n",
        "\n",
        "# Train stacked model\n",
        "stack_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = stack_model.predict(X_train)\n",
        "y_test_pred = stack_model.predict(X_test)\n",
        "\n",
        "# Training set model performance\n",
        "stack_model_train_accuracy = accuracy_score(y_train, y_train_pred) # Calculate Accuracy\n",
        "stack_model_train_mcc = matthews_corrcoef(y_train, y_train_pred) # Calculate MCC\n",
        "stack_model_train_f1 = f1_score(y_train, y_train_pred, average='weighted') # Calculate F1-score\n",
        "\n",
        "# Test set model performance\n",
        "stack_model_test_accuracy = accuracy_score(y_test, y_test_pred) # Calculate Accuracy\n",
        "stack_model_test_mcc = matthews_corrcoef(y_test, y_test_pred) # Calculate MCC\n",
        "stack_model_test_f1 = f1_score(y_test, y_test_pred, average='weighted') # Calculate F1-score\n",
        "stack_model_test_ROC = roc_auc_score(y_test, y_test_pred) # Calculate ROC AUC\n",
        "stack_model_test_recall = recall_score(y_test, y_test_pred, average='weighted') # Calculate Recall\n",
        "stack_model_test_precision = precision_score(y_test, y_test_pred, average='weighted') # Calculate Precision\n",
        "\n",
        "print('Model performance for Training set')\n",
        "print('- Accuracy: %s' % stack_model_train_accuracy)\n",
        "print('- MCC: %s' % stack_model_train_mcc)\n",
        "print('- F1 score: %s' % stack_model_train_f1)\n",
        "print('----------------------------------')\n",
        "print('Model performance for Test set')\n",
        "print('- Accuracy: %s' % stack_model_test_accuracy)\n",
        "print('- MCC: %s' % stack_model_test_mcc)\n",
        "print('- F1 score: %s' % stack_model_test_f1)\n",
        "print('- ROC AUC: %s' % stack_model_test_ROC)\n",
        "print('- Recall: %s' % stack_model_test_recall)\n",
        "print('- Precision: %s' % stack_model_test_precision)\n",
        "\n",
        "#Save model as pickle file\n",
        "filename = 'stack_model1_2Mer.sav'\n",
        "pickle.dump(stack_model, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model performance for Training set\n",
            "- Accuracy: 1.0\n",
            "- MCC: 1.0\n",
            "- F1 score: 1.0\n",
            "----------------------------------\n",
            "Model performance for Test set\n",
            "- Accuracy: 0.5645161290322581\n",
            "- MCC: 0.1272823676500175\n",
            "- F1 score: 0.5641754636833916\n",
            "- ROC AUC: 0.5635416666666666\n",
            "- Recall: 0.5645161290322581\n",
            "- Precision: 0.5641284929382816\n"
          ]
        }
      ],
      "source": [
        "# Define estimators\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "estimator_list = [\n",
        "    ('knn',knn),\n",
        "    ('poly_kernel_svm_clf',poly_kernel_svm_clf),\n",
        "    ('rf',rf) ]\n",
        "\n",
        "# Build stack model\n",
        "stack_model = StackingClassifier(\n",
        "    estimators=estimator_list, final_estimator=LogisticRegression()\n",
        ")\n",
        "\n",
        "# Train stacked model\n",
        "stack_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = stack_model.predict(X_train)\n",
        "y_test_pred = stack_model.predict(X_test)\n",
        "\n",
        "# Training set model performance\n",
        "stack_model_train_accuracy = accuracy_score(y_train, y_train_pred) # Calculate Accuracy\n",
        "stack_model_train_mcc = matthews_corrcoef(y_train, y_train_pred) # Calculate MCC\n",
        "stack_model_train_f1 = f1_score(y_train, y_train_pred, average='weighted') # Calculate F1-score\n",
        "\n",
        "# Test set model performance\n",
        "stack_model_test_accuracy = accuracy_score(y_test, y_test_pred) # Calculate Accuracy\n",
        "stack_model_test_mcc = matthews_corrcoef(y_test, y_test_pred) # Calculate MCC\n",
        "stack_model_test_f1 = f1_score(y_test, y_test_pred, average='weighted') # Calculate F1-score\n",
        "stack_model_test_ROC = roc_auc_score(y_test, y_test_pred) # Calculate ROC AUC\n",
        "stack_model_test_recall = recall_score(y_test, y_test_pred, average='weighted') # Calculate Recall\n",
        "stack_model_test_precision = precision_score(y_test, y_test_pred, average='weighted') # Calculate Precision\n",
        "\n",
        "print('Model performance for Training set')\n",
        "print('- Accuracy: %s' % stack_model_train_accuracy)\n",
        "print('- MCC: %s' % stack_model_train_mcc)\n",
        "print('- F1 score: %s' % stack_model_train_f1)\n",
        "print('----------------------------------')\n",
        "print('Model performance for Test set')\n",
        "print('- Accuracy: %s' % stack_model_test_accuracy)\n",
        "print('- MCC: %s' % stack_model_test_mcc)\n",
        "print('- F1 score: %s' % stack_model_test_f1)\n",
        "print('- ROC AUC: %s' % stack_model_test_ROC)\n",
        "print('- Recall: %s' % stack_model_test_recall)\n",
        "print('- Precision: %s' % stack_model_test_precision)\n",
        "\n",
        "#Save model as pickle file\n",
        "filename = 'stack_model2_2Mer.sav'\n",
        "pickle.dump(stack_model, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WO_qR3303OUp",
        "outputId": "985e12c1-9e17-4bd1-f9ca-8d705b47afd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model performance for Training set\n",
            "- Accuracy: 1.0\n",
            "- MCC: 1.0\n",
            "- F1 score: 1.0\n",
            "----------------------------------\n",
            "Model performance for Test set\n",
            "- Accuracy: 0.5645161290322581\n",
            "- MCC: 0.1272823676500175\n",
            "- F1 score: 0.5641754636833916\n",
            "- ROC AUC: 0.5635416666666666\n",
            "- Recall: 0.5645161290322581\n",
            "- Precision: 0.5641284929382816\n"
          ]
        }
      ],
      "source": [
        "# Define estimators\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "estimator_list = [\n",
        "    ('knn',knn),\n",
        "    ('svm_rbf',svm_rbf),\n",
        "    ('poly_kernel_svm_clf',poly_kernel_svm_clf),\n",
        "    ('rf',rf) ]\n",
        "\n",
        "# Build stack model\n",
        "stack_model = StackingClassifier(\n",
        "    estimators=estimator_list, final_estimator=LogisticRegression()\n",
        ")\n",
        "\n",
        "# Train stacked model\n",
        "stack_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = stack_model.predict(X_train)\n",
        "y_test_pred = stack_model.predict(X_test)\n",
        "\n",
        "# Training set model performance\n",
        "stack_model_train_accuracy = accuracy_score(y_train, y_train_pred) # Calculate Accuracy\n",
        "stack_model_train_mcc = matthews_corrcoef(y_train, y_train_pred) # Calculate MCC\n",
        "stack_model_train_f1 = f1_score(y_train, y_train_pred, average='weighted') # Calculate F1-score\n",
        "\n",
        "# Test set model performance\n",
        "stack_model_test_accuracy = accuracy_score(y_test, y_test_pred) # Calculate Accuracy\n",
        "stack_model_test_mcc = matthews_corrcoef(y_test, y_test_pred) # Calculate MCC\n",
        "stack_model_test_f1 = f1_score(y_test, y_test_pred, average='weighted') # Calculate F1-score\n",
        "stack_model_test_ROC = roc_auc_score(y_test, y_test_pred) # Calculate ROC AUC\n",
        "stack_model_test_recall = recall_score(y_test, y_test_pred, average='weighted') # Calculate Recall\n",
        "stack_model_test_precision = precision_score(y_test, y_test_pred, average='weighted') # Calculate Precision\n",
        "\n",
        "print('Model performance for Training set')\n",
        "print('- Accuracy: %s' % stack_model_train_accuracy)\n",
        "print('- MCC: %s' % stack_model_train_mcc)\n",
        "print('- F1 score: %s' % stack_model_train_f1)\n",
        "print('----------------------------------')\n",
        "print('Model performance for Test set')\n",
        "print('- Accuracy: %s' % stack_model_test_accuracy)\n",
        "print('- MCC: %s' % stack_model_test_mcc)\n",
        "print('- F1 score: %s' % stack_model_test_f1)\n",
        "print('- ROC AUC: %s' % stack_model_test_ROC)\n",
        "print('- Recall: %s' % stack_model_test_recall)\n",
        "print('- Precision: %s' % stack_model_test_precision)\n",
        "\n",
        "#Save model as pickle file\n",
        "filename = 'stack_model3_2Mer.sav'\n",
        "pickle.dump(stack_model, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Stacking-Classifier.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.9",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "ce0fc99d3feae300fa692521e710fc8ac7d999bfa22db95d812a9b4071499cf3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
